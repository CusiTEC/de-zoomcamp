{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVCymT5e3doj",
        "outputId": "cb3028c4-705f-4bb6-c0d6-eba89a9cfe97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=c596d9c299223ee0d84ab66e7a43425bc650bfd2bd1e1e95dacdd7086bcfd7b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xse2AEWJ3fgS",
        "outputId": "9e942e13-d9a6-43a6-f443-f9eef08adcd5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
            "      /_/\n",
            "                        \n",
            "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.22\n",
            "Branch HEAD\n",
            "Compiled by user heartsavior on 2024-02-15T11:24:58Z\n",
            "Revision fd86f85e181fc2dc0f50a096855acf83a6cc5d9c\n",
            "Url https://github.com/apache/spark\n",
            "Type --help for more information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 1\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark Test\").getOrCreate()\n",
        "spark.version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uBvDBH3q3hiy",
        "outputId": "1c93c00d-9704-4dc3-f577-99224b86a47c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.5.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "from pyspark.sql.functions import col ,unix_timestamp, max"
      ],
      "metadata": {
        "id": "kYru4_ZD3xMY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el schema del csv fhv\n",
        "schema = StructType([\n",
        "    StructField(\"dispatching_base_num\", StringType(), True),\n",
        "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
        "    StructField(\"dropOff_datetime\", TimestampType(), True),\n",
        "    StructField(\"PUlocationID\", IntegerType(), True),\n",
        "    StructField(\"DOlocationID\", IntegerType(), True),\n",
        "    StructField(\"SR_Flag\", StringType(), True),\n",
        "    StructField(\"Affiliated_base_number\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "3CUfAux730Tm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/fhv_tripdata_2019-10.csv\""
      ],
      "metadata": {
        "id": "dR8h--KO4n0b"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer el archivo con Spark\n",
        "df = spark.read.csv(file_path, header=True, schema=schema)\n",
        "\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6zbWHfC5lOL",
        "outputId": "3140e231-9981-4558-8334-5c2c6da48058"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dispatching_base_num: string (nullable = true)\n",
            " |-- pickup_datetime: timestamp (nullable = true)\n",
            " |-- dropOff_datetime: timestamp (nullable = true)\n",
            " |-- PUlocationID: integer (nullable = true)\n",
            " |-- DOlocationID: integer (nullable = true)\n",
            " |-- SR_Flag: string (nullable = true)\n",
            " |-- Affiliated_base_number: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 1\n",
        "import os\n",
        "\n",
        "df_repartitioned = df.repartition(6)\n",
        "\n",
        "# Carpeta de salida para archivos Parquet\n",
        "parquet_output_path = \"/content/parquet_repartition\"\n",
        "\n",
        "# Carpeta de salida para archivos Parquet\n",
        "df_repartitioned.write.parquet(parquet_output_path, mode=\"overwrite\")\n"
      ],
      "metadata": {
        "id": "SuP6HkWg506l"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_files = [file for file in os.listdir(parquet_output_path) if file.endswith(\".parquet\")]\n",
        "\n",
        "total_size = 0\n",
        "for file in parquet_files:\n",
        "    total_size += os.path.getsize(os.path.join(parquet_output_path, file))\n",
        "    print(total_size)\n",
        "\n",
        "# Imprimir el tamaño total\n",
        "print(f\"Tamaño total de archivos Parquet: {total_size / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "# Calcular y imprimir el tamaño promedio de los archivos Parquet\n",
        "average_size = total_size / len(parquet_files) if len(parquet_files) > 0 else 0\n",
        "print(f\"Tamaño promedio de archivos Parquet: {average_size / (1024 * 1024):.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIFR6w3MrcVG",
        "outputId": "deaf14af-3fdc-4802-b426-5c5c13e99bd6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6548003\n",
            "13081421\n",
            "19623186\n",
            "26163076\n",
            "32719544\n",
            "39257481\n",
            "Tamaño total de archivos Parquet: 37.44 MB\n",
            "Tamaño promedio de archivos Parquet: 6.24 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the repartitioned DataFrame to Parquet format\n",
        "parquet_output_path = \"/content/parquet_output_case\"\n",
        "df.write.parquet(parquet_output_path, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "lIzxyizEAf6G"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 3\n",
        "# Leer los archivos Parquet en un DataFrame\n",
        "df_parquet = spark.read.parquet(parquet_output_path)\n",
        "df_parquet.show()\n",
        "# Filtrar los viajes que comenzaron el 15 de octubre\n",
        "trips_oct_15 = df_parquet.filter(col(\"pickup_datetime\").cast(\"date\") == \"2019-10-15 00:00:00\")\n",
        "\n",
        "# Contar el número de viajes\n",
        "count_oct_15 = trips_oct_15.count()\n",
        "\n",
        "# Imprimir el resultado\n",
        "print(f\"Número de viajes en taxi el 15 de octubre: {count_oct_15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "813lQWrWAqqX",
        "outputId": "b4042fab-9410-4755-9aa5-4440d3ea3267"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
            "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
            "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
            "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   NULL|                B00009|\n",
            "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   NULL|                B00013|\n",
            "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   NULL|                B00014|\n",
            "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   NULL|                B00014|\n",
            "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   NULL|                B00014|\n",
            "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   NULL|       B00021         |\n",
            "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   NULL|       B00021         |\n",
            "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   NULL|       B00021         |\n",
            "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   NULL|       B00021         |\n",
            "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   NULL|       B00021         |\n",
            "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   NULL|       B00021         |\n",
            "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   NULL|                B00037|\n",
            "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   NULL|                B00037|\n",
            "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   NULL|                B00037|\n",
            "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   NULL|                B00037|\n",
            "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   NULL|                B00037|\n",
            "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   NULL|                B00037|\n",
            "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   NULL|                B00037|\n",
            "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   NULL|                B00037|\n",
            "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   NULL|                  #N/A|\n",
            "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Número de viajes en taxi el 15 de octubre: 62610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 4\n",
        "df_parquet = spark.read.parquet(parquet_output_path)\n",
        "\n",
        "# Calcular la duración del viaje en segundos\n",
        "df_with_duration = df_parquet.withColumn(\n",
        "    \"trip_duration_seconds\",\n",
        "    (unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\"))\n",
        ")\n",
        "\n",
        "# Encontrar la duración máxima del viaje\n",
        "max_duration = df_with_duration.agg(max(\"trip_duration_seconds\")).collect()[0][0]\n",
        "\n",
        "# Convertir la duración máxima de segundos a horas\n",
        "max_duration_hours = max_duration / 3600\n",
        "\n",
        "# Imprimir el resultado\n",
        "print(f\"Duración del viaje más largo en horas: {max_duration_hours}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4S9o3DBCMSc",
        "outputId": "9c6347ba-6526-4c63-fc6d-3cfe022e4ac9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duración del viaje más largo en horas: 631152.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 5\n",
        "# Cargar los datos de búsqueda de zona en una vista temporal\n",
        "zone_data_url = \"/content/taxi_zone_lookup.csv\"\n",
        "zone_data = spark.read.option(\"header\", \"true\").csv(zone_data_url)\n",
        "zone_data.createOrReplaceTempView(\"zone_data\")\n",
        "\n",
        "# Ruta al archivo de datos FHV de octubre de 2019\n",
        "fhv_data_url = \"/content/fhv_tripdata_2019-10.csv\"\n",
        "\n",
        "# Leer los datos FHV en un DataFrame\n",
        "fhv_data = spark.read.csv(fhv_data_url, header=True)\n",
        "\n",
        "# Realizar un join entre los datos FHV y los datos de búsqueda de zona\n",
        "joined_data = fhv_data.join(\n",
        "    zone_data,\n",
        "    fhv_data.PUlocationID == zone_data.LocationID,\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "# Encontrar la zona de recogida menos frecuente\n",
        "least_frequent_zone = joined_data.groupBy(\"Zone\").count().orderBy(col(\"count\")).first()\n",
        "\n",
        "# Imprimir el nombre de la zona de recogida menos frecuente\n",
        "print(f\"Nombre de la zona de recogida menos frecuente: {least_frequent_zone['Zone']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfUvyYZRDMjb",
        "outputId": "2091f940-97f8-44a4-c95e-221ba16805d1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de la zona de recogida menos frecuente: Jamaica Bay\n"
          ]
        }
      ]
    }
  ]
}